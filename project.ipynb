{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import preprocessing\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word2vec(description):\n",
    "    description = description_list[0]\n",
    "    description = description.lower()\n",
    "    description = re.sub('[^a-zA-Z]', ' ', description )  \n",
    "    description = re.sub(r'\\s+', ' ', description)\n",
    "    description_tokens = nltk.sent_tokenize(description)\n",
    "    all_words = [nltk.word_tokenize(sent) for sent in description_tokens]\n",
    "    for i in range(len(all_words)):  \n",
    "        all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]\n",
    "    all_words = all_words[0]\n",
    "    avg_word2vec_words = np.zeros(300)\n",
    "    for word in all_words:\n",
    "        avg_word2vec_words += model[word]\n",
    "        \n",
    "    return avg_word2vec_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_col_word2vec(column):\n",
    "    col_vector_list = []\n",
    "    for description in column:\n",
    "        description_vector = get_word2vec(description)\n",
    "        col_vector_list.append(description_vector)\n",
    "    return np.array(col_vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _run_nn(train_feature_matrix, train_label_matrix, test_feature_matrix):\n",
    "    nn_clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(3000), random_state=1)\n",
    "    nn_train_feature_matrix = train_feature_matrix.astype(np.float64)\n",
    "    nn_test_feature_matrix = test_feature_matrix.astype(np.float64)\n",
    "    nn_clf.fit(nn_train_feature_matrix, train_label_matrix)\n",
    "    nn_predictions = nn_clf.predict(nn_test_feature_matrix)\n",
    "    return nn_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _run_svm(train_feature_matrix, train_label_matrix, test_feature_matrix):\n",
    "    clf = SVC(gamma='auto')\n",
    "    clf.fit(train_feature_matrix, train_label_matrix)\n",
    "    predicted_labels = clf.predict(test_feature_matrix)\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _run_dtree(train_feature_matrix, train_label_matrix, test_feature_matrix):\n",
    "    dt_clf = tree.DecisionTreeClassifier()\n",
    "    dt_clf = dt_clf.fit(train_feature_matrix, train_label_matrix)\n",
    "    dt_predictions = dt_clf.predict(test_feature_matrix)\n",
    "    return dt_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_macro_f1_score(predictions, true_labels):\n",
    "    true_positives = [0 for i in range(11)]\n",
    "    false_positives = [0 for i in range(11)]\n",
    "    false_negatives = [0 for i in range(11)]\n",
    "\n",
    "    if len(predictions) != len(true_labels):\n",
    "        print(\"bug in code, length of predictions should match length of true_labels\")\n",
    "        return None\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == true_labels[i]:\n",
    "            true_positives[predictions[i]] += 1\n",
    "        else:\n",
    "            false_positives[predictions[i]] += 1\n",
    "            false_negatives[true_labels[i]] += 1\n",
    "\n",
    "    total_classes = 0\n",
    "    total_f1 = 0\n",
    "    for i in range(11):\n",
    "        if true_positives[i]==0 and false_positives[i]==0:\n",
    "            continue\n",
    "        elif true_positives[i]==0 and false_negatives[i]==0:\n",
    "            continue\n",
    "        prec = true_positives[i]*1.0/(true_positives[i] + false_positives[i])\n",
    "        recall = true_positives[i]*1.0/(true_positives[i]+false_negatives[i])\n",
    "        f1=0\n",
    "        if prec+recall != 0:\n",
    "            f1 = 2*prec*recall/(prec+recall)\n",
    "            total_classes += 1\n",
    "            total_f1 += f1\n",
    "    return total_f1/total_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_vector_column = convert_text_col_word2vec(train_data['Name'])\n",
    "description_vector_column = convert_text_col_word2vec(train_data['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_columns = ['Name','Description','PetID']\n",
    "dataset = train_data.drop(delete_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.join(pd.DataFrame(name_vector_column), rsuffix='_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.join(pd.DataFrame(description_vector_column), rsuffix='_description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_columns = ['Breed1','Breed2','Type','Gender','Color1','Color2','Color3','Vaccinated','Dewormed','Sterilized',\n",
    "                   'Health','MaturitySize','State','RescuerID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in one_hot_columns:\n",
    "    dataset=pd.get_dummies(dataset, columns=[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_dataset(t_frac, random_state, dataset):\n",
    "    testset=dataset.sample(frac=t_frac,random_state=random_state)\n",
    "    trainset=dataset.drop(testset.index)\n",
    "    testset.to_csv(\"testSet.csv\", index = False)\n",
    "    trainset.to_csv(\"trainingSet.csv\", index = False)\n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset, validationset = split_dataset(0.3, 47, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features_labels(dataset):\n",
    "    labels = dataset['AdoptionSpeed']\n",
    "    features = dataset.drop(['AdoptionSpeed'], axis=1)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_matrix, train_label_matrix = get_features_labels(trainset)\n",
    "validation_features, validation_labels = get_features_labels(validationset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4498"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_predictions(model_name, train_features_matrix, train_label_matrix, test_features):\n",
    "    predicted_labels = np.zeros(len(test_features))\n",
    "    if model_name == 'nn':\n",
    "        predicted_labels = _run_nn(train_features_matrix, train_label_matrix, test_features)\n",
    "    elif model_name == 'svm':\n",
    "        predicted_labels = _run_svm(train_features_matrix, train_label_matrix, test_features)\n",
    "    elif model_name == 'dtree':\n",
    "        predicted_labels = _run_dtree(train_features_matrix, train_label_matrix, test_features)\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm time taken 1455.3647270202637\n",
      "svm micro f1 score 0.3330369052912405\n",
      "svm accuracy score 0.3330369052912405\n",
      "svm macro f1 score 21.756816451122297\n",
      "dtree time taken 2.8952572345733643\n",
      "dtree micro f1 score 0.3750555802578924\n",
      "dtree accuracy score 0.3750555802578924\n",
      "dtree macro f1 score 32.05676590219869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohitgupta/anaconda2/envs/python36/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn time taken 253.57829523086548\n",
      "nn micro f1 score 0.40929301911960875\n",
      "nn accuracy score 0.4092930191196087\n",
      "nn macro f1 score 37.295981169032785\n"
     ]
    }
   ],
   "source": [
    "models = ['svm','dtree','nn']\n",
    "predicted_labels_dict = {}\n",
    "\n",
    "for model in models:\n",
    "    start_time = time.time()\n",
    "    predicted_labels = get_predictions(model, train_features_matrix, train_label_matrix, validation_features)\n",
    "    predicted_labels_dict[model]=predicted_labels\n",
    "    end_time = time.time() - start_time\n",
    "    print (model, \"time taken\", end_time)\n",
    "    print (model, \"micro f1 score\", f1_score(predicted_labels, validation_labels.values,average='micro'))\n",
    "    print (model, \"accuracy score\", accuracy_score(predicted_labels, validation_labels.values))\n",
    "    print (model, \"macro f1 score\", calculate_macro_f1_score(predicted_labels, validation_labels.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohitgupta/anaconda2/envs/python36/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn time taken 5728.024206161499\n",
      "nn micro f1 score 0.4255224544241885\n",
      "nn accuracy score 0.4255224544241885\n",
      "nn macro f1 score 38.12221005040227\n"
     ]
    }
   ],
   "source": [
    "models = ['nn']\n",
    "predicted_labels_dict = {}\n",
    "\n",
    "for model in models:\n",
    "    start_time = time.time()\n",
    "    predicted_labels = get_predictions(model, train_features_matrix, train_label_matrix, validation_features)\n",
    "    predicted_labels_dict[model]=predicted_labels\n",
    "    end_time = time.time() - start_time\n",
    "    print (model, \"time taken\", end_time)\n",
    "    print (model, \"micro f1 score\", f1_score(predicted_labels, validation_labels.values,average='micro'))\n",
    "    print (model, \"accuracy score\", accuracy_score(predicted_labels, validation_labels.values))\n",
    "    print (model, \"macro f1 score\", calculate_macro_f1_score(predicted_labels, validation_labels.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10495, 5967)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
